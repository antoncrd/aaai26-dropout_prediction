{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee626bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run brazil_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e976ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb0f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "predict_students_dropout_and_academic_success = fetch_ucirepo(id=697) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df1 = predict_students_dropout_and_academic_success.data.features \n",
    "y_uci = predict_students_dropout_and_academic_success.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a96ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['email'] = df1.index\n",
    "df1['source'] = 'real'\n",
    "mapping = {'Dropout': 1, 'Graduate': 0, 'Enrolled': 0}\n",
    "df1['dropout'] = y_uci['Target'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28869509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     229\n",
      "1     105\n",
      "2     129\n",
      "3     383\n",
      "4     168\n",
      "5      24\n",
      "6     266\n",
      "7     110\n",
      "8     547\n",
      "9     292\n",
      "10    521\n",
      "11     77\n",
      "12    201\n",
      "13     13\n",
      "14    411\n",
      "15     51\n",
      "16    506\n",
      "17     72\n",
      "18     92\n",
      "19    227\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 20\n",
    "\n",
    "X = df1.drop(columns=['email', 'dropout', 'source']).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df2 = df1.copy()\n",
    "df2['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47db1f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters trop petits à réaffecter : [1, 2, 5, 7, 11, 13, 15, 17, 18]\n",
      "Nouvelles tailles de clusters :\n",
      " 0     277\n",
      "3     401\n",
      "4     180\n",
      "6     303\n",
      "8     644\n",
      "9     458\n",
      "10    567\n",
      "12    279\n",
      "14    461\n",
      "16    579\n",
      "19    275\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "min_cluster_size = 150\n",
    "df3 = assign_clusters_with_min_size(df1, n_clusters=20, min_cluster_size=min_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181d0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = augment_minority_clusters(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf8bf8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-5.29) | Discrim. (-0.48): 100%|██████████| 50/50 [06:47<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "ctgan1 = CTGAN(\n",
    "    epochs=50,\n",
    "    batch_size=100,\n",
    "    generator_dim=(256, 256),\n",
    "    discriminator_dim=(256, 256),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. Entraînement\n",
    "# Le modèle apprendra la distribution de vos données\n",
    "X = df1.copy()\n",
    "X = X.drop(columns=['email', 'source']).fillna(0)\n",
    "ctgan1.fit(X)\n",
    "\n",
    "synthetic_data = ctgan1.sample(n_samples)\n",
    "synthetic_data['source'] = 'synth'\n",
    "\n",
    "df5 = pd.concat([df1, synthetic_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    (\"sans cluster\", df1, {}),\n",
    "    (\"sans enrichissement\", df3, {}),\n",
    "    ((\"avec SMOTE\", df4, {})),\n",
    "    # ,(\"avec enrichissement, calcul déterministe\", df4, {\"globe\": False}), # globe=False\n",
    "    # (\"avec GAN\", df5, {})\n",
    "]\n",
    "\n",
    "summary_records = []\n",
    "res = []\n",
    "for name, dfk, kwargs in tasks:\n",
    "    # 1) Lancement de la fonction\n",
    "    df_detail, df_agg, y_all, trained_clfs = run_analysis_bra(\n",
    "    df=dfk.drop(columns=['dropout']),\n",
    "    y=dfk['dropout'],\n",
    "    alpha=0.05,\n",
    "    nan_fill=0,\n",
    "    do_plot=False\n",
    ")\n",
    "    r =  df_detail.groupby([\"method\", \"model\", \"cluster\", \"n_projects\"]).agg(\n",
    "            mean_coverage=(\"coverage\", \"mean\"),\n",
    "            mean_width=(\"width\", \"mean\")\n",
    "        ).reset_index()\n",
    "    res.append(r[r['cluster']==-1])\n",
    "    print(res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a520a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous qualification\n",
      "Curricular units 1st sem\n",
      "Curricular units 2nd sem\n"
     ]
    }
   ],
   "source": [
    "curri_cols = ['Previous qualification', \"Curricular units 1st sem\", \"Curricular units 2nd sem\"]\n",
    "\n",
    "# curri_cols = ['Previous qualification','Admission grade', \"Curricular units 1st sem\", \"Curricular units 2nd sem\"]\n",
    "dyn_cols = [\n",
    "        col for col in df1.columns\n",
    "        if any(col.startswith(pref) for pref in curri_cols)\n",
    "        ]\n",
    "static_cols = [\n",
    "c for c in df1.columns\n",
    "if c not in dyn_cols + [\"student_id\", \"email\", \"dropout\", \"source\", \"cluster\"]\n",
    "]\n",
    "\n",
    "# 3. DataFrame de base, qu’on ne modifie pas en place\n",
    "base_df = df1[static_cols].copy()\n",
    "\n",
    "# 4. Construction cumulative du dictionnaire Xt\n",
    "Xt = {\"t0\": base_df.copy()}\n",
    "cum_df = base_df.copy()\n",
    "for idx, prefix in enumerate(curri_cols, start=1):\n",
    "    print(prefix)\n",
    "    cum_df = cum_df.copy()\n",
    "    cols = [c for c in df1.columns if c.startswith(prefix)]\n",
    "    cum_df[cols] = df1[cols]\n",
    "    Xt[f\"t{idx}\"] = cum_df\n",
    "\n",
    "# 5. Construire y pour l’horizon H\n",
    "H = 1\n",
    "keys = list(Xt.keys())\n",
    "y = {}\n",
    "\n",
    "# cible pour t0\n",
    "if H < len(keys):\n",
    "    y[\"t0\"] = Xt[keys[H]].iloc[:, -1].copy()\n",
    "else:\n",
    "    y[\"t0\"] = Xt[keys[-1]].iloc[:, -1].copy()\n",
    "\n",
    "for i, key in enumerate(keys[1:], start=1):\n",
    "    tgt = i + H\n",
    "    if tgt < len(keys):\n",
    "        df_tgt = Xt[keys[tgt]]\n",
    "    else:\n",
    "        df_tgt = Xt[keys[-1]]\n",
    "    y[key] = df_tgt.iloc[:, -2].copy()\n",
    "\n",
    "# 6. Construire X en ne gardant que les w dernières notes (fenêtre glissante)\n",
    "w = 1\n",
    "X = {}\n",
    "\n",
    "# on parcourt les mêmes clés que pour y, mais on saute celles où i < w\n",
    "for i, key in enumerate(keys):\n",
    "    if i < w:\n",
    "        continue\n",
    "    # on prend les w derniers item_cols correspondant aux notes t_{i-w+1} … t_i\n",
    "    window_item_cols = dyn_cols[i-w : i]\n",
    "    X[key] = df1[static_cols + window_item_cols].copy()\n",
    "\n",
    "# 7. Conversion en arrays NumPy (alignés sur les mêmes clés)\n",
    "valid_keys = keys[w:]  # on commence à t{w}\n",
    "X_array_hori = [X[k].values for k in valid_keys]\n",
    "y_array_hori = [y[k].values for k in valid_keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e261a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t0', 't1', 't2', 't3', 't4']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ade0fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  50%|█████     | 1/2 [11:11<11:11, 671.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fenêtre 1: cov=0.8341, excès=0.1299, witdh=6.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne: 100%|██████████| 2/2 [34:24<00:00, 1032.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fenêtre 2: cov=0.8730, excès=0.4166, witdh=4.9132\n",
      "   fenêtre     cov  excess\n",
      "0        1  0.8341  0.1299\n",
      "1        2  0.8730  0.4166\n",
      "\n",
      "Couverture moyenne  : 0.8536\n",
      "Excès moyen (width) : 0.2732\n",
      "   fenêtre     cov  excess\n",
      "0        1  0.8341  0.1299\n",
      "1        2  0.8730  0.4166\n",
      "\n",
      "Couverture moyenne  : 0.8536\n",
      "Excès moyen (width) : 0.2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covs, ecs, models, witdh = [], [], [], []\n",
    "X_array = X_array_hori\n",
    "k = len(X_array)\n",
    "keys = list(Xt.keys())\n",
    "# Parcours des fenêtres temporelles\n",
    "for i in tqdm(range(1, k), desc=\"Fenêtres en ligne\"):\n",
    "\n",
    "    H = len(keys) - i - w\n",
    "    y = {}\n",
    "\n",
    "    # cible pour t0\n",
    "    if H < len(keys):\n",
    "        y[\"t0\"] = Xt[keys[H]].iloc[:, -1].copy()\n",
    "    else:\n",
    "        y[\"t0\"] = Xt[keys[-1]].iloc[:, -1].copy()\n",
    "\n",
    "    for j, key in enumerate(keys[1:], start=1):\n",
    "        tgt = j + H\n",
    "        if tgt < len(keys):\n",
    "            df_tgt = Xt[keys[tgt]]\n",
    "        else:\n",
    "            df_tgt = Xt[keys[-1]]\n",
    "        y[key] = df_tgt.iloc[:, -2].copy()\n",
    "    y_array = [y[k].values for k in valid_keys]\n",
    "# On parcourt i de 1 à len(X_array)-1 (i=0 n'a pas de passé pour entraîner)\n",
    "    # --- 1) Construction du train sur les fenêtres passées ---\n",
    "    X_train = np.vstack(X_array[:i])      # fenêtres 0..i-1\n",
    "    y_train = np.concatenate(y_array[:i])\n",
    "    # --- 2) Entraînement d'un nouveau modèle ---\n",
    "    model = TwoSidedSPCI_RFQuant_Offline(alpha=0.05, w=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    # --- 3) Évaluation sur la fenêtre courante i ---\n",
    "    X_i, y_i = X_array[i], y_array[i]\n",
    "    # calcul des bornes supérieures U_t pour chaque échantillon de X_i\n",
    "    L = np.array([\n",
    "        model.predict_interval(x.reshape(1, -1))[0]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    U = np.array([\n",
    "        model.predict_interval(x.reshape(1, -1))[1]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    covs.append(np.mean((U >= y_i) & (L <= y_i)))\n",
    "    ecs.append(np.mean(np.maximum(0, y_i - U)) + np.mean(np.maximum(0, L - y_i)))\n",
    "    witdh.append(np.mean(U - L))\n",
    "    print(f\"Fenêtre {i}: cov={covs[-1]:.4f}, excès={ecs[-1]:.4f}, witdh={witdh[-1]:.4f}\")\n",
    "\n",
    "# 4) Rapport final\n",
    "report = (\n",
    "    pd.DataFrame({\n",
    "        \"fenêtre\": np.arange(1, k),\n",
    "        \"cov\"    : covs,\n",
    "        \"excess\" : ecs,\n",
    "    })\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(report)\n",
    "print(f\"\\nCouverture moyenne  : {report['cov'].mean():.4f}\")\n",
    "print(f\"Excès moyen (width) : {report['excess'].mean():.4f}\")\n",
    "print(report)\n",
    "print(f\"\\nCouverture moyenne  : {report['cov'].mean():.4f}\")\n",
    "print(f\"Excès moyen (width) : {report['excess'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b932185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import os\n",
    "from mapie.metrics import (\n",
    "    classification_coverage_score,\n",
    "    classification_mean_width_score\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from typing import Tuple, Dict, Union, List\n",
    "from mapie.classification import MapieClassifier\n",
    "from mapie.mondrian import MondrianCP\n",
    "from mapie.metrics import (\n",
    "    classification_coverage_score,\n",
    "    classification_mean_width_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    brier_score_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ff4ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(pset):\n",
    "    res = []\n",
    "    for pred in pset:\n",
    "        res.append(np.mean(pred))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f3791ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  50%|█████     | 1/2 [01:18<01:18, 78.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTAT : 0.733499095840868 0.3188456312467857\n",
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne: 100%|██████████| 2/2 [03:34<00:00, 107.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTAT : 0.7954339963833634 0.1962198893213437\n",
      "   fenêtre     cov  excess\n",
      "0        1  0.7335  0.3188\n",
      "1        2  0.7954  0.1962\n",
      "\n",
      "Couverture moyenne  : 0.7645\n",
      "Excès moyen (width) : 0.2575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covs, ecs = [], []\n",
    "X_array = X_array_hori\n",
    "y_array = y_array_hori\n",
    "U_t = []\n",
    "# On parcourt i de 1 à len(X_array)-1 (i=0 n'a pas de passé pour entraîner)\n",
    "for i in tqdm(range(1, len(X_array)), desc=\"Fenêtres en ligne\"):\n",
    "    # --- 1) Construction du train sur les fenêtres passées ---\n",
    "    X_train = np.vstack(X_array[:i])      # fenêtres 0..i-1\n",
    "    y_train = np.concatenate(y_array[:i])\n",
    "    # --- 2) Entraînement d'un nouveau modèle ---\n",
    "    model_one = OneSidedSPCI_LGBM_Offline(alpha=0.1, w=200, random_state=0)\n",
    "    model_one.fit(X_train, y_train)\n",
    "\n",
    "    # --- 3) Évaluation sur la fenêtre courante i ---\n",
    "    X_i, y_i = X_array[i], y_array[i]\n",
    "    # calcul des bornes supérieures U_t pour chaque échantillon de X_i\n",
    "    U = np.array([\n",
    "        model_one.predict_interval(x.reshape(1, -1))[1]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    U_t.append(U)\n",
    "    # coverage & excess\n",
    "    covs.append(np.mean(U >= y_i))\n",
    "    ecs.append(np.mean(np.maximum(0, y_i - U)))\n",
    "    print(\"RESULTAT :\", np.mean(U >= y_i), np.mean(np.maximum(0, y_i - U)))\n",
    "# --- 4) Rapport final ---\n",
    "report = (\n",
    "    pd.DataFrame({\n",
    "        \"fenêtre\": np.arange(1, len(X_array)),\n",
    "        \"cov\"    : covs,\n",
    "        \"excess\" : ecs,\n",
    "    })\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(report)\n",
    "print(f\"\\nCouverture moyenne  : {report['cov'].mean():.4f}\")\n",
    "print(f\"Excès moyen (width) : {report['excess'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33527f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfng = df1.copy()\n",
    "idx2 = dfng.columns.get_loc('Curricular units 1st sem (without evaluations)')\n",
    "dfng.insert(idx2+1, \"Curricular units 1st sem (next grade)\", U_t[1])\n",
    "idx1 = dfng.columns.get_loc('Previous qualification (grade)')\n",
    "dfng.insert(idx1+1, \"Previous qualification (next grade)\", U_t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3862954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     368\n",
      "1     208\n",
      "2     182\n",
      "3     228\n",
      "4     177\n",
      "5     401\n",
      "6     295\n",
      "7     344\n",
      "8     189\n",
      "9     107\n",
      "10     98\n",
      "11    194\n",
      "12     87\n",
      "13    351\n",
      "14    234\n",
      "15    449\n",
      "16    279\n",
      "17     20\n",
      "18     74\n",
      "19    139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dfng.drop(columns=['email', 'dropout', 'source']).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "dfng1 = dfng.copy()\n",
    "dfng1['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dfc27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters trop petits à réaffecter : [9, 10, 12, 17, 18, 19]\n",
      "Nouvelles tailles de clusters :\n",
      " 0     417\n",
      "1     234\n",
      "2     290\n",
      "3     318\n",
      "4     180\n",
      "5     419\n",
      "6     328\n",
      "7     350\n",
      "8     241\n",
      "11    211\n",
      "13    402\n",
      "14    255\n",
      "15    474\n",
      "16    305\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dfng2 = assign_clusters_with_min_size(dfng, n_clusters=20, min_cluster_size=min_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26f3830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfng3 = augment_minority_clusters(dfng1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7102bd0",
   "metadata": {},
   "source": [
    "## COMBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e0c32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP [0.9627118644067797] [1.3141242937853108]\n",
      "SPCI [0.9807909604519774] [1.7966101694915255]\n",
      "UNION : 0.9977401129943503 1.848587570621469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF:  50%|█████     | 1/2 [01:27<01:27, 87.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED 0.9468926553672317 1.2666666666666666\n",
      "MCP [0.9627118644067797, 0.9627118644067797] [1.3141242937853108, 1.3231638418079097]\n",
      "SPCI [0.9807909604519774, 0.9491525423728814] [1.7966101694915255, 1.4903954802259887]\n",
      "UNION : 0.9830508474576272 1.5909604519774012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF: 100%|██████████| 2/2 [02:59<00:00, 89.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED 0.9299435028248587 1.2271186440677966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP [0.9694915254237289] [1.3887005649717514]\n",
      "SPCI [0.9807909604519774] [1.7966101694915255]\n",
      "UNION : 0.9977401129943503 1.856497175141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR:  50%|█████     | 1/2 [01:16<01:16, 76.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED 0.9536723163841808 1.3299435028248587\n",
      "MCP [0.9694915254237289, 0.9694915254237289] [1.3887005649717514, 1.3954802259887005]\n",
      "SPCI [0.9807909604519774, 0.9491525423728814] [1.7966101694915255, 1.4903954802259887]\n",
      "UNION : 0.9864406779661017 1.6282485875706214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR: 100%|██████████| 2/2 [02:11<00:00, 65.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED 0.9355932203389831 1.264406779661017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GB:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP [0.9706214689265537] [1.3333333333333333]\n",
      "SPCI [0.9807909604519774] [1.7966101694915255]\n",
      "UNION : 0.9988700564971752 1.8519774011299435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GB:  50%|█████     | 1/2 [00:43<00:43, 43.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED 0.9525423728813559 1.280225988700565\n",
      "MCP [0.9706214689265537, 0.9706214689265537] [1.3333333333333333, 1.3322033898305086]\n",
      "SPCI [0.9807909604519774, 0.9491525423728814] [1.7966101694915255, 1.4903954802259887]\n",
      "UNION : 0.9864406779661017 1.6033898305084746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GB: 100%|██████████| 2/2 [01:28<00:00, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED 0.9344632768361582 1.2350282485875705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration & constants\n",
    "# -----------------------------------------------------------------------------\n",
    "RANDOM_STATE: int = 42            # Ensures full reproducibility\n",
    "ALPHA: float = 0.05               # Target mis-coverage level\n",
    "W: int = 1                        # Sliding-window size\n",
    "nan_fill = 0\n",
    "threshold = 10.001\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data preparation\n",
    "# -----------------------------------------------------------------------------\n",
    "DF = dfng2.copy()\n",
    "DF.fillna(nan_fill, inplace=True)\n",
    "DF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "curri_cols = [\n",
    "    'Previous qualification',\n",
    "    'Curricular units 1st sem',\n",
    "    'Curricular units 2nd sem'\n",
    "]\n",
    "static_cols = [\n",
    "    c for c in DF.columns\n",
    "    if c not in curri_cols + [\"student_id\", \"email\", \"dropout\", \"source\", \"cluster\"]\n",
    "]\n",
    "\n",
    "Y_TARGET = DF['dropout'].astype(int)\n",
    "DF.drop(columns='dropout', inplace=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Base models\n",
    "# -----------------------------------------------------------------------------\n",
    "MODELS: Dict[str, object] = {\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"LR\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"GB\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Conformal prediction evaluation loop\n",
    "# -----------------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "res_fin: List[pd.DataFrame] = []\n",
    "\n",
    "for name, base_clf in MODELS.items():\n",
    "    covs_MCP, width_MCP = [], []\n",
    "    covs_SPCI, width_SPCI = [], []\n",
    "    covs_comb, width_comb = [], []\n",
    "    covs_union,  width_union  = [], []\n",
    "    for n in tqdm(range(W, len(curri_cols)), desc=name):\n",
    "        gate_clf = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=None,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        # 1) split train / tmp / test\n",
    "        idx_tmp, idx_test, y_tmp, y_test, cl_tmp, cl_test = train_test_split(\n",
    "            DF.index, Y_TARGET, DF[\"cluster\"],\n",
    "            test_size=0.20,\n",
    "            stratify=Y_TARGET,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        idx_tr, idx_cal, y_tr, y_cal, cl_tr, cl_cal = train_test_split(\n",
    "            idx_tmp, y_tmp, cl_tmp,\n",
    "            test_size=0.40/0.80,\n",
    "            stratify=y_tmp,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        # **nouveau** split en deux pour CP vs gate\n",
    "        idx_cal_cp, idx_cal_gate, y_cal_cp, y_cal_gate, cl_cal_cp, cl_cal_gate = train_test_split(\n",
    "            idx_cal, y_cal, cl_cal,\n",
    "            test_size=0.5,\n",
    "            stratify=y_cal,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        y_cal_cp   = np.array(y_cal_cp)\n",
    "        y_cal_gate = np.array(y_cal_gate)\n",
    "        y_test     = np.array(y_test)\n",
    "        # mask des “réels” pour toutes les évaluations\n",
    "        mask_real = DF.loc[idx_test, \"source\"] == \"real\"\n",
    "\n",
    "        # 2) build features\n",
    "        X_tr        = build_X_(DF.loc[idx_tr],       curri_cols, static_cols, n)\n",
    "        X_cal_cp    = build_X_(DF.loc[idx_cal_cp],   curri_cols, static_cols, n)\n",
    "        X_cal_gate  = build_X_(DF.loc[idx_cal_gate], curri_cols, static_cols, n)\n",
    "        X_test      = build_X_(DF.loc[idx_test],     curri_cols, static_cols, n)\n",
    "        # 3) train base clf + calibrate for MCP\n",
    "        clf = clone(base_clf)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        calib = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\") \\\n",
    "                    .fit(X_cal_cp, y_cal_cp)\n",
    "\n",
    "        base_mapie = MapieClassifier(estimator=calib, method=\"lac\", cv=\"prefit\")\n",
    "        mond_mapie = MondrianCP(mapie_estimator=base_mapie) \\\n",
    "                        .fit(X_cal_cp, y_cal_cp, partition=cl_cal_cp)\n",
    "\n",
    "        # ---- MCP on TEST ----\n",
    "        _, yps_van_test = mond_mapie.predict(X_test, alpha=ALPHA, partition=cl_test)\n",
    "        pset_van_test = yps_van_test[:, :, 0]\n",
    "        cov_van = classification_coverage_score(y_test[mask_real], pset_van_test[mask_real])\n",
    "        wid_van = classification_mean_width_score(pset_van_test[mask_real])\n",
    "        covs_MCP.append(cov_van)\n",
    "        width_MCP.append(wid_van)\n",
    "        print(\"MCP\", cov_van, wid_van)\n",
    "        # ---- SPCI on TEST ----\n",
    "        model_spci = loaded_models[n - W]\n",
    "        pos_test   = DF.index.get_indexer(idx_test)\n",
    "        X_spci_test = X_array_hori[n - W + 1][pos_test]\n",
    "        intervals = [model_spci.predict_interval(x.reshape(1, -1))\n",
    "                     for x in X_spci_test]\n",
    "        L_preds, U_preds = zip(*intervals)\n",
    "\n",
    "        y_pred_bool_SPCI = np.zeros((len(intervals), 2), dtype=bool)\n",
    "        for i, (L, U) in enumerate(zip(L_preds, U_preds)):\n",
    "            if threshold > U:\n",
    "                y_pred_bool_SPCI[i, 1] = True\n",
    "            elif threshold < L:\n",
    "                y_pred_bool_SPCI[i, 0] = True\n",
    "            else:\n",
    "                y_pred_bool_SPCI[i, :] = True\n",
    "\n",
    "        cov_spci = classification_coverage_score(y_test[mask_real],\n",
    "                                                y_pred_bool_SPCI[mask_real])\n",
    "        wid_spci = classification_mean_width_score(y_pred_bool_SPCI[mask_real])\n",
    "        covs_SPCI.append(cov_spci)\n",
    "        width_SPCI.append(wid_spci)\n",
    "        print(\"SPCI\", cov_spci, wid_spci)\n",
    "        ##UNION \n",
    "        y_pred_bool_MCP = pset_van_test.astype(bool)\n",
    "        y_bool_union = y_pred_bool_MCP | y_pred_bool_SPCI\n",
    "        cov_union = classification_coverage_score(\n",
    "            y_test[mask_real],\n",
    "            y_bool_union[mask_real]\n",
    "        )\n",
    "        wid_union = classification_mean_width_score(\n",
    "            y_bool_union[mask_real]\n",
    "        )\n",
    "        covs_union.append(cov_union)\n",
    "        width_union.append(wid_union)\n",
    "        print(\"UNION :\", cov_union, wid_union)\n",
    "\n",
    "        # ---- construire la gate sur CAL_GATE ----\n",
    "        #  a) MCP predictions sur X_cal_gate\n",
    "        _, yps_van_gate = mond_mapie.predict(\n",
    "            X_cal_gate, alpha=ALPHA, partition=cl_cal_gate\n",
    "        )\n",
    "        pset_cal_cls = yps_van_gate[:, :, 0]\n",
    "\n",
    "        #  b) SPCI predictions sur X_cal_gate\n",
    "        pos_cal_gate  = DF.index.get_indexer(idx_cal_gate)\n",
    "        X_spci_cal    = X_array_hori[n - W + 1][pos_cal_gate]\n",
    "        intervals_cal = [model_spci.predict_interval(x.reshape(1, -1))\n",
    "                         for x in X_spci_cal]\n",
    "        L_cal, U_cal  = zip(*intervals_cal)\n",
    "\n",
    "        pset_cal_spc = np.zeros_like(pset_cal_cls, dtype=bool)\n",
    "        for i, (L, U) in enumerate(zip(L_cal, U_cal)):\n",
    "            if threshold > U:\n",
    "                pset_cal_spc[i, 1] = True\n",
    "            elif threshold < L:\n",
    "                pset_cal_spc[i, 0] = True\n",
    "            else:\n",
    "                pset_cal_spc[i, :] = True\n",
    "\n",
    "        #  c) préparer méta-features & labels pour la gate\n",
    "        df_sel_arr = []\n",
    "        labels_g   = []                      # ← on initialise labels_g\n",
    "\n",
    "        for i in range(len(idx_cal_gate)):\n",
    "            feat_vec = X_cal_gate[i]\n",
    "            w_cls    = pset_cal_cls[i].sum()\n",
    "            w_spc    = pset_cal_spc[i].sum()\n",
    "            diff     = w_cls - w_spc\n",
    "            err_cls  = int(y_cal_gate[i] not in np.where(pset_cal_cls[i])[0])\n",
    "            err_spc  = int(y_cal_gate[i] not in np.where(pset_cal_spc[i])[0])\n",
    "            if   err_cls == 0 and err_spc == 1:\n",
    "                gate_y = 0\n",
    "            elif err_spc == 0 and err_cls == 1:\n",
    "                gate_y = 1\n",
    "            elif err_cls == 0 and err_spc == 0:\n",
    "                gate_y = 0 if w_cls < w_spc else 1\n",
    "            else:\n",
    "                gate_y = 2\n",
    "            labels_g.append(gate_y)           # ← on stocke le label\n",
    "\n",
    "            meta_vec = np.concatenate([\n",
    "                 feat_vec,\n",
    "                 [w_cls, w_spc, diff, err_cls, err_spc]\n",
    "            ])\n",
    "            df_sel_arr.append(meta_vec)\n",
    "\n",
    "        X_gate_train = np.vstack(df_sel_arr)\n",
    "        gate_clf.fit(X_gate_train, np.array(labels_g))\n",
    "        # ---- appliquer la gate sur TEST ----\n",
    "        meta_test_arr = []\n",
    "        for i in range(len(idx_test)):\n",
    "            # feat_vec est un array 1D de taille n_features\n",
    "            feat_vec = X_test[i]\n",
    "            w_cls = pset_van_test[i].sum()\n",
    "            w_spc = y_pred_bool_SPCI[i].sum()\n",
    "            diff = w_cls - w_spc\n",
    "            # on concatène feat_vec et les 5 features méta\n",
    "            meta_vec = np.concatenate([\n",
    "                feat_vec,\n",
    "                [w_cls, w_spc, diff, 0, 0]    # err_cls=0, err_spc=0\n",
    "            ])\n",
    "            meta_test_arr.append(meta_vec)\n",
    "\n",
    "        # on empile en matrice (n_test × n_features_meta)\n",
    "        X_gate_test = np.vstack(meta_test_arr)\n",
    "\n",
    "        # on prédit le choix de la gate\n",
    "        choices = gate_clf.predict(X_gate_test)\n",
    "        pset_final = np.zeros_like(pset_van_test, dtype=bool)\n",
    "        for i, choice in enumerate(choices):\n",
    "            if choice == 0:\n",
    "                pset_final[i] = y_pred_bool_MCP[i]\n",
    "            elif choice == 1:\n",
    "                pset_final[i] = y_pred_bool_SPCI[i]\n",
    "            else:\n",
    "                pset_final[i] = y_pred_bool_MCP[i] | y_pred_bool_SPCI[i]\n",
    "\n",
    "        cov_c = classification_coverage_score(y_test[mask_real],\n",
    "                                             pset_final[mask_real])\n",
    "        wid_c = classification_mean_width_score(pset_final[mask_real])\n",
    "        covs_comb.append(cov_c)\n",
    "        width_comb.append(wid_c)\n",
    "        print(\"COMBINED\", cov_c, wid_c)\n",
    "    # on agrège les métriques\n",
    "    n_vals = list(range(W, W + len(covs_MCP)))\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"model\":             [name] * len(n_vals),\n",
    "        \"n\":                 n_vals,\n",
    "        \"coverage_MCP\":      covs_MCP,\n",
    "        \"width_MCP\":         width_MCP,\n",
    "        \"coverage_SPCI\":     covs_SPCI,\n",
    "        \"width_SPCI\":        width_SPCI,\n",
    "        \"coverage_union\":    covs_union, \n",
    "        \"width_union\":       width_union,\n",
    "        \"coverage_combined\": covs_comb,\n",
    "        \"width_combined\":    width_comb,\n",
    "    })\n",
    "    res_fin.append(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0010e962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n</th>\n",
       "      <th>coverage_MCP</th>\n",
       "      <th>width_MCP</th>\n",
       "      <th>coverage_SPCI</th>\n",
       "      <th>width_SPCI</th>\n",
       "      <th>coverage_union</th>\n",
       "      <th>width_union</th>\n",
       "      <th>coverage_combined</th>\n",
       "      <th>width_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962712</td>\n",
       "      <td>1.314124</td>\n",
       "      <td>0.980791</td>\n",
       "      <td>1.796610</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>1.848588</td>\n",
       "      <td>0.946893</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>2</td>\n",
       "      <td>0.962712</td>\n",
       "      <td>1.323164</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>1.490395</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>1.590960</td>\n",
       "      <td>0.929944</td>\n",
       "      <td>1.227119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  n  coverage_MCP  width_MCP  coverage_SPCI  width_SPCI  \\\n",
       "0    RF  1      0.962712   1.314124       0.980791    1.796610   \n",
       "1    RF  2      0.962712   1.323164       0.949153    1.490395   \n",
       "\n",
       "   coverage_union  width_union  coverage_combined  width_combined  \n",
       "0        0.997740     1.848588           0.946893        1.266667  \n",
       "1        0.983051     1.590960           0.929944        1.227119  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_fin[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a73a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n                    1.500000\n",
       "coverage_MCP         0.962712\n",
       "width_MCP            1.318644\n",
       "coverage_SPCI        0.964972\n",
       "width_SPCI           1.643503\n",
       "coverage_union       0.990395\n",
       "width_union          1.719774\n",
       "coverage_combined    0.938418\n",
       "width_combined       1.246893\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_fin[0].iloc[:,1:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d378587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n                    1.500000\n",
       "coverage_MCP         0.970621\n",
       "width_MCP            1.332768\n",
       "coverage_SPCI        0.964972\n",
       "width_SPCI           1.643503\n",
       "coverage_union       0.992655\n",
       "width_union          1.727684\n",
       "coverage_combined    0.943503\n",
       "width_combined       1.257627\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_fin[2].iloc[:,1:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c42a760a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47368421, 0.96271186, 1.31840618, 0.96580434, 1.65156111,\n",
       "       0.99078204, 1.72655367, 0.93886411, 1.24793339])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "weighted = np.average(\n",
    "    res_fin[0].iloc[:, 1:],    # toutes vos métriques\n",
    "    axis=0,\n",
    "    weights=[gamma, gamma**2]    # vos poids\n",
    ")\n",
    "weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e8b2d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n</th>\n",
       "      <th>coverage_MCP</th>\n",
       "      <th>width_MCP</th>\n",
       "      <th>coverage_SPCI</th>\n",
       "      <th>width_SPCI</th>\n",
       "      <th>coverage_union</th>\n",
       "      <th>width_union</th>\n",
       "      <th>coverage_combined</th>\n",
       "      <th>width_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970621</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.980791</td>\n",
       "      <td>1.796610</td>\n",
       "      <td>0.998870</td>\n",
       "      <td>1.851977</td>\n",
       "      <td>0.952542</td>\n",
       "      <td>1.280226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GB</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970621</td>\n",
       "      <td>1.332203</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>1.490395</td>\n",
       "      <td>0.986441</td>\n",
       "      <td>1.603390</td>\n",
       "      <td>0.934463</td>\n",
       "      <td>1.235028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  n  coverage_MCP  width_MCP  coverage_SPCI  width_SPCI  \\\n",
       "0    GB  1      0.970621   1.333333       0.980791    1.796610   \n",
       "1    GB  2      0.970621   1.332203       0.949153    1.490395   \n",
       "\n",
       "   coverage_union  width_union  coverage_combined  width_combined  \n",
       "0        0.998870     1.851977           0.952542        1.280226  \n",
       "1        0.986441     1.603390           0.934463        1.235028  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_fin[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327207c",
   "metadata": {},
   "source": [
    "## Weird Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration & constants\n",
    "# -----------------------------------------------------------------------------\n",
    "RANDOM_STATE: int = 42            # Ensures full reproducibility\n",
    "ALPHA: float = 0.05               # Target mis‑coverage level\n",
    "W: int = 1                        # Sliding‑window size (kept as plain int)\n",
    "nan_fill = 0\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data preparation (identical logic)\n",
    "# -----------------------------------------------------------------------------\n",
    "DF = dfng2.copy()            \n",
    "DF.fillna(0, inplace=True)\n",
    "DF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "curri_cols = ['Previous qualification', \"Curricular units 1st sem\", \"Curricular units 2nd sem\"]\n",
    "prefixes = curri_cols.copy()\n",
    "dyn_cols = [\n",
    "    col for col in DF.columns\n",
    "    if any(col.startswith(pref) for pref in prefixes)\n",
    "    ]\n",
    "static_cols = [\n",
    "c for c in DF.columns\n",
    "if c not in dyn_cols + [\"student_id\", \"email\", \"dropout\", \"source\", \"cluster\"]\n",
    "]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model definitions (identical hyper‑parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "MODELS: Dict[str, object] = {\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"LR\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"GB\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper functions (ported verbatim where possible)\n",
    "# -----------------------------------------------------------------------------\n",
    "loaded_models = models\n",
    "N_RENDUS: int = 1\n",
    "\n",
    "threshold = 10.001\n",
    "Y_TARGET = DF['dropout'].astype(int)\n",
    "DF.drop(columns='dropout', inplace=True)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Conformal prediction evaluation loop (logic unchanged)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "records: List[dict] = []\n",
    "trained_clfs: Dict[Tuple[str, int, str], Union[MapieClassifier, MondrianCP]] = {}\n",
    "\n",
    "clusters_all = DF[\"cluster\"]\n",
    "idx_all = DF.index.values\n",
    "\n",
    "res_fin = []\n",
    "for name, base_clf in MODELS.items():\n",
    "    covs_MCP, width_MCP = [], []\n",
    "    covs_SPCI, width_SPCI = [], []\n",
    "    meta_rows = []\n",
    "    ACC = []\n",
    "    covs_combine, width_combine = [], []\n",
    "    for n in tqdm(range(W, len(prefixes)), desc=name):  # noqa: F821 – prefixes provided\n",
    "        clf = clone(base_clf)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Dataset splitting\n",
    "        # ------------------------------------------------------------------\n",
    "        idx_tmp, idx_test, y_tmp, y_test, cl_tmp, cl_test = train_test_split(\n",
    "            idx_all,\n",
    "            Y_TARGET,\n",
    "            clusters_all,\n",
    "            test_size=0.20,\n",
    "            stratify=Y_TARGET,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        idx_tr, idx_cal, y_tr, y_cal, cl_tr, cl_cal = train_test_split(\n",
    "            idx_tmp,\n",
    "            y_tmp,\n",
    "            cl_tmp,\n",
    "            test_size=0.20 / 0.80,\n",
    "            stratify=y_tmp,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Feature extraction (delegated to user‑supplied `build_X`)\n",
    "        # ------------------------------------------------------------------\n",
    "        X_tr = build_X_(DF.loc[idx_tr], prefixes, static_cols, n)     # noqa: F821\n",
    "        X_cal = build_X_(DF.loc[idx_cal], prefixes, static_cols, n)\n",
    "        X_test = build_X_(DF.loc[idx_test], prefixes, static_cols, n)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Training & calibration\n",
    "        # ------------------------------------------------------------------\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        calib = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\").fit(X_cal, y_cal)\n",
    "\n",
    "        base_mapie = MapieClassifier(estimator=calib, method=\"lac\", cv=\"prefit\")\n",
    "        mond_mapie = MondrianCP(mapie_estimator=base_mapie).fit(X_cal, y_cal, partition=cl_cal)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # MCP evaluation\n",
    "        # ------------------------------------------------------------------\n",
    "        mask_real = DF.loc[idx_test, \"source\"] == \"real\"\n",
    "        _, yps_van = mond_mapie.predict(X_test, alpha=ALPHA, partition=cl_test)\n",
    "        pset_van = yps_van[:, :, 0]\n",
    "        # print(pset_van)\n",
    "        RES_MCP = encode(pset_van)\n",
    "        # print(RES_MCP)\n",
    "        cov_van_MCP = classification_coverage_score(y_test[mask_real], pset_van[mask_real])\n",
    "        width_van_MCP = classification_mean_width_score(pset_van[mask_real])\n",
    "        print(\"MCP :\", cov_van_MCP, width_van_MCP)\n",
    "\n",
    "        covs_MCP.append(cov_van_MCP)\n",
    "        width_MCP.append(width_van_MCP)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # SPCI evaluation\n",
    "        # ------------------------------------------------------------------\n",
    "        model_SPCI = loaded_models[n - W]                       # noqa: F821\n",
    "        pos_test = DF.index.get_indexer(idx_test)\n",
    "        X_test_SPCI = X_array_hori[n - W + 1][pos_test]         # noqa: F821\n",
    "\n",
    "        intervals = [model_SPCI.predict_interval(x.reshape(1, -1)) for x in X_test_SPCI]\n",
    "        L_preds, U_preds = zip(*intervals)\n",
    "\n",
    "        y_preds_SPCI: List[List[int]] = []\n",
    "        for L, U in zip(L_preds, U_preds):\n",
    "            if threshold > U:\n",
    "                y_preds_SPCI.append([1])\n",
    "            elif threshold < L:\n",
    "                y_preds_SPCI.append([0])\n",
    "            else:\n",
    "                y_preds_SPCI.append([0, 1])\n",
    "\n",
    "        RES_SPCI = encode(y_preds_SPCI)\n",
    "\n",
    "        y_pred_bool_SPCI = np.zeros((len(y_preds_SPCI), 2), dtype=bool)\n",
    "        for i_row, labels in enumerate(y_preds_SPCI):\n",
    "            y_pred_bool_SPCI[i_row, labels] = True\n",
    "\n",
    "        cov_van_SPCI = classification_coverage_score(y_test[mask_real], y_pred_bool_SPCI[mask_real])\n",
    "        width_van_SPCI = classification_mean_width_score(y_pred_bool_SPCI[mask_real])\n",
    "        print(\"SPCI :\", cov_van_SPCI, width_van_SPCI)\n",
    "\n",
    "        covs_SPCI.append(cov_van_SPCI)\n",
    "        width_SPCI.append(width_van_SPCI)\n",
    "        META_DF = pd.DataFrame({\n",
    "            \"id\": idx_test,\n",
    "            \"res_mcp\": RES_MCP,\n",
    "            \"res_spci\": RES_SPCI,\n",
    "            \"y\": y_test.values\n",
    "        })\n",
    "\n",
    "\n",
    "        # META_DF = pd.concat(meta_rows, ignore_index=True)\n",
    "        X_meta = META_DF[[\"res_mcp\", \"res_spci\"]]\n",
    "        y_meta = META_DF[\"y\"]\n",
    "\n",
    "\n",
    "        meta_clf_n = LogisticRegression(\n",
    "            class_weight=\"balanced\", max_iter=1000, random_state=RANDOM_STATE\n",
    "        ).fit(X_meta, y_meta)\n",
    "        X_fin = build_X_(DF, prefixes, static_cols, n)\n",
    "        _, yps_van_all = mond_mapie.predict(X_fin, alpha=ALPHA, partition=clusters_all)\n",
    "        pset_van_all = yps_van_all[:, :, 0]\n",
    "        RES_MCP_test = encode(pset_van_all)\n",
    "\n",
    "        X_all_SPCI = X_array_hori[n - W + 1]    \n",
    "\n",
    "        intervals_all = [model_SPCI.predict_interval(x.reshape(1, -1)) for x in X_all_SPCI]\n",
    "        L_preds_all, U_preds_all = zip(*intervals_all)\n",
    "\n",
    "        y_preds_SPCI_all: List[List[int]] = []\n",
    "        for L, U in zip(L_preds_all, U_preds_all):\n",
    "            if threshold > U:\n",
    "                y_preds_SPCI_all.append([1])\n",
    "            elif threshold < L:\n",
    "                y_preds_SPCI_all.append([0])\n",
    "            else:\n",
    "                y_preds_SPCI_all.append([0, 1])\n",
    "\n",
    "        RES_SPCI_test = encode(y_preds_SPCI_all)\n",
    "        META_DF_test = (pd.DataFrame({\n",
    "            \"res_mcp\": RES_MCP_test,\n",
    "            \"res_spci\": RES_SPCI_test,\n",
    "            \"y\": Y_TARGET.values\n",
    "        }))\n",
    "        X_meta_test = META_DF_test[[\"res_mcp\", \"res_spci\"]]\n",
    "        y_meta_test = META_DF_test[\"y\"]\n",
    "\n",
    "        y_pred_fin = meta_clf_n.predict_proba(X_meta_test)[:,1]\n",
    "        threshold = 0.50                       # à adapter selon vos besoins\n",
    "        y_pred_bin = (y_pred_fin >= threshold).astype(int)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 2) Métriques principales\n",
    "        # ----------------------------------------------------------\n",
    "        acc       = accuracy_score(y_meta_test, y_pred_bin)\n",
    "        print(\"accuracy : \", acc)\n",
    "        ACC.append(acc)\n",
    "        pset_combine = []\n",
    "        for y_pr in y_pred_fin:\n",
    "            if y_pr > 0.7:\n",
    "              pset_combine.append([1])\n",
    "            elif y_pr < 0.3:\n",
    "                pset_combine.append([0])\n",
    "            else:\n",
    "                pset_combine.append([0, 1])\n",
    "        gamma_pred_combine = np.zeros((len(pset_combine), 2), dtype=bool)\n",
    "        for i_row, labels in enumerate(pset_combine):\n",
    "            gamma_pred_combine[i_row, labels] = True\n",
    "        cov_van_combine = classification_coverage_score(Y_TARGET.values, gamma_pred_combine)\n",
    "        width_van_combine = classification_mean_width_score(gamma_pred_combine)\n",
    "        print(\"combine :\", cov_van_combine, width_van_combine)\n",
    "        covs_combine.append(cov_van_combine)\n",
    "        width_combine.append(width_van_combine)\n",
    "    df_metrics_model = pd.DataFrame({\n",
    "    \"model\": [name] * len(range(W, len(prefixes))),\n",
    "    \"n\": list(range(W, len(prefixes))),\n",
    "    \"coverage_MCP\": covs_MCP,\n",
    "    \"width_MCP\": width_MCP,\n",
    "    \"coverage_SPCI\": covs_SPCI,\n",
    "    \"width_SPCI\": width_SPCI,\n",
    "    \"coverage_combined\": covs_combine,\n",
    "    \"width_combined\": width_combine,\n",
    "    \"accuracy_combined\": ACC\n",
    "    })\n",
    "    res_fin.append(df_metrics_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
