{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee626bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run portugal_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e976ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from ctgan import CTGAN\n",
    "# Standard library\n",
    "from typing import List, Dict\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Conformal prediction (MAPIE + Mondrian CP)\n",
    "from mapie.classification import MapieClassifier\n",
    "from mapie.metrics import classification_coverage_score, classification_mean_width_score\n",
    "from mapie.mondrian import MondrianCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "predict_students_dropout_and_academic_success = fetch_ucirepo(id=697) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df1 = predict_students_dropout_and_academic_success.data.features \n",
    "y_uci = predict_students_dropout_and_academic_success.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a96ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['email'] = df1.index\n",
    "df1['source'] = 'real'\n",
    "mapping = {'Dropout': 1, 'Graduate': 0, 'Enrolled': 0}\n",
    "df1['dropout'] = y_uci['Target'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28869509",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "\n",
    "X = df1.drop(columns=['email', 'dropout', 'source']).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df2 = df1.copy()\n",
    "df2['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 150\n",
    "df3 = assign_clusters_with_min_size(df1, n_clusters=20, min_cluster_size=min_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = augment_minority_clusters(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "ctgan1 = CTGAN(\n",
    "    epochs=300,\n",
    "    batch_size=100,\n",
    "    generator_dim=(256, 256),\n",
    "    discriminator_dim=(256, 256),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. Entraînement\n",
    "# Le modèle apprendra la distribution de vos données\n",
    "X = df1.copy()\n",
    "X = X.drop(columns=['email', 'source']).fillna(0)\n",
    "ctgan1.fit(X)\n",
    "\n",
    "synthetic_data = ctgan1.sample(n_samples)\n",
    "synthetic_data['source'] = 'synth'\n",
    "\n",
    "df5 = pd.concat([df1, synthetic_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    (\"without cluster\", df1, {}),\n",
    "    (\"without enrichissement\", df3, {}),\n",
    "    ((\"with SMOTE\", df4, {})),\n",
    "    (\"with GAN\", df5, {})\n",
    "]\n",
    "\n",
    "summary_records = []\n",
    "res = []\n",
    "for name, dfk, kwargs in tasks:\n",
    "    # 1) Lancement de la fonction\n",
    "    df_detail, df_agg, y_all, trained_clfs = run_analysis_portugal(\n",
    "    df=dfk.drop(columns=['dropout']),\n",
    "    y=dfk['dropout'],\n",
    "    alpha=0.05,\n",
    "    nan_fill=0,\n",
    "    do_plot=False\n",
    ")\n",
    "    r =  df_detail.groupby([\"method\", \"model\", \"cluster\", \"n_projects\"]).agg(\n",
    "            mean_coverage=(\"coverage\", \"mean\"),\n",
    "            mean_width=(\"width\", \"mean\")\n",
    "        ).reset_index()\n",
    "    res.append(r[r['cluster']==-1])\n",
    "    print(res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curri_cols = ['Previous qualification', \"Curricular units 1st sem\", \"Curricular units 2nd sem\"]\n",
    "\n",
    "# curri_cols = ['Previous qualification','Admission grade', \"Curricular units 1st sem\", \"Curricular units 2nd sem\"]\n",
    "dyn_cols = [\n",
    "        col for col in df1.columns\n",
    "        if any(col.startswith(pref) for pref in curri_cols)\n",
    "        ]\n",
    "static_cols = [\n",
    "c for c in df1.columns\n",
    "if c not in dyn_cols + [\"student_id\", \"email\", \"dropout\", \"source\", \"cluster\"]\n",
    "]\n",
    "\n",
    "# 3. DataFrame de base, qu’on ne modifie pas en place\n",
    "base_df = df1[static_cols].copy()\n",
    "\n",
    "# 4. Construction cumulative du dictionnaire Xt\n",
    "Xt = {\"t0\": base_df.copy()}\n",
    "cum_df = base_df.copy()\n",
    "for idx, prefix in enumerate(curri_cols, start=1):\n",
    "    print(prefix)\n",
    "    cum_df = cum_df.copy()\n",
    "    cols = [c for c in df1.columns if c.startswith(prefix)]\n",
    "    cum_df[cols] = df1[cols]\n",
    "    Xt[f\"t{idx}\"] = cum_df\n",
    "\n",
    "# 5. Construire y pour l’horizon H\n",
    "H = 1\n",
    "keys = list(Xt.keys())\n",
    "y = {}\n",
    "\n",
    "# cible pour t0\n",
    "if H < len(keys):\n",
    "    y[\"t0\"] = Xt[keys[H]].iloc[:, -1].copy()\n",
    "else:\n",
    "    y[\"t0\"] = Xt[keys[-1]].iloc[:, -1].copy()\n",
    "\n",
    "for i, key in enumerate(keys[1:], start=1):\n",
    "    tgt = i + H\n",
    "    if tgt < len(keys):\n",
    "        df_tgt = Xt[keys[tgt]]\n",
    "    else:\n",
    "        df_tgt = Xt[keys[-1]]\n",
    "    y[key] = df_tgt.iloc[:, -2].copy()\n",
    "\n",
    "# 6. Construire X en ne gardant que les w dernières notes (fenêtre glissante)\n",
    "w = 1\n",
    "X = {}\n",
    "\n",
    "# on parcourt les mêmes clés que pour y, mais on saute celles où i < w\n",
    "for i, key in enumerate(keys):\n",
    "    if i < w:\n",
    "        continue\n",
    "    # on prend les w derniers item_cols correspondant aux notes t_{i-w+1} … t_i\n",
    "    window_item_cols = dyn_cols[i-w : i]\n",
    "    X[key] = df1[static_cols + window_item_cols].copy()\n",
    "\n",
    "# 7. Conversion en arrays NumPy (alignés sur les mêmes clés)\n",
    "valid_keys = keys[w:]  # on commence à t{w}\n",
    "X_array_hori = [X[k].values for k in valid_keys]\n",
    "y_array_hori = [y[k].values for k in valid_keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "covs, ecs, models2sp, witdh = [], [], [], []\n",
    "X_array = X_array_hori\n",
    "k = len(X_array)\n",
    "keys = list(Xt.keys())\n",
    "# Parcours des fenêtres temporelles\n",
    "for i in tqdm(range(1, k), desc=\"Fenêtres en ligne\"):\n",
    "\n",
    "    H = len(keys) - i - w\n",
    "    y = {}\n",
    "\n",
    "    # cible pour t0\n",
    "    if H < len(keys):\n",
    "        y[\"t0\"] = Xt[keys[H]].iloc[:, -1].copy()\n",
    "    else:\n",
    "        y[\"t0\"] = Xt[keys[-1]].iloc[:, -1].copy()\n",
    "\n",
    "    for j, key in enumerate(keys[1:], start=1):\n",
    "        tgt = j + H\n",
    "        if tgt < len(keys):\n",
    "            df_tgt = Xt[keys[tgt]]\n",
    "        else:\n",
    "            df_tgt = Xt[keys[-1]]\n",
    "        y[key] = df_tgt.iloc[:, -2].copy()\n",
    "    y_array = [y[k].values for k in valid_keys]\n",
    "# On parcourt i de 1 à len(X_array)-1 (i=0 n'a pas de passé pour entraîner)\n",
    "    # --- 1) Construction du train sur les fenêtres passées ---\n",
    "    X_train = np.vstack(X_array[:i])      # fenêtres 0..i-1\n",
    "    y_train = np.concatenate(y_array[:i])\n",
    "    # --- 2) Entraînement d'un nouveau modèle ---\n",
    "    model = TwoSidedSPCI_RFQuant_Offline(alpha=0.05, w=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    models2sp.append(model)\n",
    "    # --- 3) Évaluation sur la fenêtre courante i ---\n",
    "    X_i, y_i = X_array[i], y_array[i]\n",
    "    # calcul des bornes supérieures U_t pour chaque échantillon de X_i\n",
    "    L = np.array([\n",
    "        model.predict_interval(x.reshape(1, -1))[0]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    U = np.array([\n",
    "        model.predict_interval(x.reshape(1, -1))[1]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    covs.append(np.mean((U >= y_i) & (L <= y_i)))\n",
    "    ecs.append(np.mean(np.maximum(0, y_i - U)) + np.mean(np.maximum(0, L - y_i)))\n",
    "    witdh.append(np.mean(U - L))\n",
    "    print(f\"Fenêtre {i}: cov={covs[-1]:.4f}, excès={ecs[-1]:.4f}, witdh={witdh[-1]:.4f}\")\n",
    "\n",
    "# 4) Rapport final\n",
    "report = (\n",
    "    pd.DataFrame({\n",
    "        \"fenêtre\": np.arange(1, k),\n",
    "        \"cov\"    : covs,\n",
    "        \"excess\" : ecs,\n",
    "    })\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(report)\n",
    "print(f\"\\nCouverture moyenne  : {report['cov'].mean():.4f}\")\n",
    "print(f\"Excès moyen (width) : {report['excess'].mean():.4f}\")\n",
    "print(report)\n",
    "print(f\"\\nCouverture moyenne  : {report['cov'].mean():.4f}\")\n",
    "print(f\"Excès moyen (width) : {report['excess'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3791ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "covs, ecs = [], []\n",
    "X_array = X_array_hori\n",
    "y_array = y_array_hori\n",
    "U_t = []\n",
    "# On parcourt i de 1 à len(X_array)-1 (i=0 n'a pas de passé pour entraîner)\n",
    "for i in tqdm(range(1, len(X_array)), desc=\"Fenêtres en ligne\"):\n",
    "    # --- 1) Construction du train sur les fenêtres passées ---\n",
    "    X_train = np.vstack(X_array[:i])      # fenêtres 0..i-1\n",
    "    y_train = np.concatenate(y_array[:i])\n",
    "    # --- 2) Entraînement d'un nouveau modèle ---\n",
    "    model_one = OneSidedSPCI_LGBM_Offline(alpha=0.1, w=200, random_state=0)\n",
    "    model_one.fit(X_train, y_train)\n",
    "\n",
    "    # --- 3) Évaluation sur la fenêtre courante i ---\n",
    "    X_i, y_i = X_array[i], y_array[i]\n",
    "    # calcul des bornes supérieures U_t pour chaque échantillon de X_i\n",
    "    U = np.array([\n",
    "        model_one.predict_interval(x.reshape(1, -1))[1]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    U_t.append(U)\n",
    "    # coverage & excess\n",
    "    covs.append(np.mean(U >= y_i))\n",
    "    ecs.append(np.mean(np.maximum(0, y_i - U)))\n",
    "    print(\"RESULTAT :\", np.mean(U >= y_i), np.mean(np.maximum(0, y_i - U)))\n",
    "# --- 4) Rapport final ---\n",
    "report = (\n",
    "    pd.DataFrame({\n",
    "        \"fenêtre\": np.arange(1, len(X_array)),\n",
    "        \"cov\"    : covs,\n",
    "        \"excess\" : ecs,\n",
    "    })\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(report)\n",
    "print(f\"\\nCouverture moyenne  : {report['cov'].mean():.4f}\")\n",
    "print(f\"Excès moyen (width) : {report['excess'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33527f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfng = df1.copy()\n",
    "idx2 = dfng.columns.get_loc('Curricular units 1st sem (without evaluations)')\n",
    "dfng.insert(idx2+1, \"Curricular units 1st sem (next grade)\", U_t[1])\n",
    "idx1 = dfng.columns.get_loc('Previous qualification (grade)')\n",
    "dfng.insert(idx1+1, \"Previous qualification (next grade)\", U_t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3862954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfng.drop(columns=['email', 'dropout', 'source']).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "dfng1 = dfng.copy()\n",
    "dfng1['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfng2 = assign_clusters_with_min_size(dfng, n_clusters=20, min_cluster_size=min_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfng3 = augment_minority_clusters(dfng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "ctgan2 = CTGAN(\n",
    "    epochs=300,\n",
    "    batch_size=100,\n",
    "    generator_dim=(256, 256),\n",
    "    discriminator_dim=(256, 256),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. Entraînement\n",
    "# Le modèle apprendra la distribution de vos données\n",
    "X = dfng1.copy()\n",
    "X = X.drop(columns=['email', 'source']).fillna(0)\n",
    "ctgan2.fit(X)\n",
    "\n",
    "synthetic_data = ctgan2.sample(n_samples)\n",
    "synthetic_data['source'] = 'synth'\n",
    "\n",
    "dfng4 = pd.concat([dfng1, synthetic_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    (\"without cluster\", dfng, {}),\n",
    "    (\"without enrichment\", dfng2, {}),\n",
    "    ((\"with SMOTE\", dfng3, {})),\n",
    "    (\"avec GAN\", dfng4, {})\n",
    "]\n",
    "\n",
    "summary_records = []\n",
    "res = []\n",
    "for name, dfk, kwargs in tasks:\n",
    "    # 1) Lancement de la fonction\n",
    "    df_detail, df_agg, y_all, trained_clfs = run_analysis_portugal(\n",
    "    df=dfk.drop(columns=['dropout']),\n",
    "    y=dfk['dropout'],\n",
    "    alpha=0.05,\n",
    "    nan_fill=0,\n",
    "    do_plot=False\n",
    ")\n",
    "    r =  df_detail.groupby([\"method\", \"model\", \"cluster\", \"n_projects\"]).agg(\n",
    "            mean_coverage=(\"coverage\", \"mean\"),\n",
    "            mean_width=(\"width\", \"mean\")\n",
    "        ).reset_index()\n",
    "    res.append(r[r['cluster']==-1])\n",
    "    print(res[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7102bd0",
   "metadata": {},
   "source": [
    "## COMBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e317183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_cal_with_real_priority(\n",
    "        idx_tmp: np.ndarray,\n",
    "        y_tmp: np.ndarray,\n",
    "        src_tmp: np.ndarray,\n",
    "        cal_fraction: float = 0.25,  # 0.20/0.80 = 0.25 comme avant\n",
    "        random_state: int = 42,\n",
    "    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Retourne (idx_tr, idx_cal, y_tr, y_cal) en assurant :\n",
    "        - stratification sur y\n",
    "        - la calibration prélève d'abord dans source=='real' puis complète sur le reste\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        n_total = len(idx_tmp)\n",
    "        n_cal = int(round(n_total * cal_fraction))\n",
    "\n",
    "        df_tmp_split = pd.DataFrame({\n",
    "            \"idx\": idx_tmp,\n",
    "            \"y\": y_tmp,\n",
    "            \"source\": src_tmp\n",
    "        })\n",
    "\n",
    "        chosen_idx = []\n",
    "\n",
    "        # Allocation par classe pour garder l'équilibre\n",
    "        for cls, grp in df_tmp_split.groupby(\"y\"):\n",
    "            n_cls = len(grp)\n",
    "            n_cal_cls = int(round(n_cal * n_cls / n_total))\n",
    "\n",
    "            grp_real = grp[grp[\"source\"] == \"real\"]\n",
    "            grp_other = grp[grp[\"source\"] != \"real\"]\n",
    "\n",
    "            # échantillonnage d'abord dans le réel\n",
    "            take_real = min(len(grp_real), n_cal_cls)\n",
    "            part_real = grp_real.sample(n=take_real, random_state=rng) if take_real > 0 else grp_real\n",
    "\n",
    "            remaining = n_cal_cls - take_real\n",
    "            if remaining > 0:\n",
    "                part_other = grp_other.sample(n=min(remaining, len(grp_other)), random_state=rng) if len(grp_other) > 0 else grp_other\n",
    "                chosen_idx.append(pd.concat([part_real[\"idx\"], part_other[\"idx\"]], ignore_index=True))\n",
    "            else:\n",
    "                chosen_idx.append(part_real[\"idx\"])\n",
    "\n",
    "        idx_cal = pd.Index(np.concatenate([c.values for c in chosen_idx])) if len(chosen_idx) else pd.Index([])\n",
    "\n",
    "        # Si léger décalage dû aux arrondis, on complète en priorisant à nouveau 'real'\n",
    "        if len(idx_cal) < n_cal:\n",
    "            remaining_df = df_tmp_split[~df_tmp_split[\"idx\"].isin(idx_cal)]\n",
    "            remaining_df = pd.concat([\n",
    "                remaining_df[remaining_df[\"source\"] == \"real\"],\n",
    "                remaining_df[remaining_df[\"source\"] != \"real\"]\n",
    "            ], ignore_index=True)\n",
    "            add_n = min(n_cal - len(idx_cal), len(remaining_df))\n",
    "            if add_n > 0:\n",
    "                extra = remaining_df.sample(n=add_n, random_state=rng)[\"idx\"]\n",
    "                idx_cal = pd.Index(np.concatenate([idx_cal.values, extra.values]))\n",
    "\n",
    "        idx_cal = np.unique(idx_cal)\n",
    "        mask_tr = ~np.isin(idx_tmp, idx_cal)\n",
    "        idx_tr = idx_tmp[mask_tr]\n",
    "\n",
    "        y_tr = y_tmp[mask_tr]\n",
    "        y_cal = y_tmp[np.isin(idx_tmp, idx_cal)]\n",
    "\n",
    "        return idx_tr, idx_cal, y_tr, y_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration & constants\n",
    "# -----------------------------------------------------------------------------\n",
    "RANDOM_STATE: int = 42            # Ensures full reproducibility\n",
    "ALPHA: float = 0.05               # Target mis-coverage level\n",
    "W: int = 1                        # Sliding-window size\n",
    "nan_fill = 0\n",
    "threshold = 10.001\n",
    "loaded_models = models2sp\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data preparation\n",
    "# -----------------------------------------------------------------------------\n",
    "DF = dfng2.copy()\n",
    "DF.fillna(nan_fill, inplace=True)\n",
    "DF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "curri_cols = [\n",
    "    'Previous qualification',\n",
    "    'Curricular units 1st sem',\n",
    "    'Curricular units 2nd sem'\n",
    "]\n",
    "static_cols = [\n",
    "    c for c in DF.columns\n",
    "    if c not in curri_cols + [\"student_id\", \"email\", \"dropout\", \"source\", \"cluster\"]\n",
    "]\n",
    "\n",
    "Y_TARGET = DF['dropout'].astype(int)\n",
    "DF.drop(columns='dropout', inplace=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Base models\n",
    "# -----------------------------------------------------------------------------\n",
    "MODELS: Dict[str, object] = {\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"LR\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"GB\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Conformal prediction evaluation loop\n",
    "# -----------------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "res_fin: List[pd.DataFrame] = []\n",
    "\n",
    "for name, base_clf in MODELS.items():\n",
    "    covs_MCP, width_MCP = [], []\n",
    "    covs_SPCI, width_SPCI = [], []\n",
    "    covs_comb, width_comb = [], []\n",
    "    covs_union,  width_union  = [], []\n",
    "    for n in tqdm(range(W, len(curri_cols)), desc=name):\n",
    "        gate_clf = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=None,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        # 1) split train / tmp / test\n",
    "        idx_tmp, idx_test, y_tmp, y_test, cl_tmp, cl_test = train_test_split(\n",
    "            DF.index, Y_TARGET, DF[\"cluster\"],\n",
    "            test_size=0.20,\n",
    "            stratify=Y_TARGET,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        src_all = DF[\"source\"].values\n",
    "        src_tmp = src_all[idx_tmp]\n",
    "        idx_tr, idx_cal, y_tr, y_cal = split_train_cal_with_real_priority(\n",
    "            idx_tmp=idx_tmp,\n",
    "            y_tmp=y_tmp,\n",
    "            src_tmp=src_tmp,\n",
    "            cal_fraction=0.40 / 0.80,  # conserve la même taille de calibration qu'avant\n",
    "            random_state=42,\n",
    "        )\n",
    "        # On aligne aussi les clusters correspondants\n",
    "        mask_tr = np.isin(idx_tmp, idx_tr)\n",
    "        mask_cal = np.isin(idx_tmp, idx_cal)\n",
    "        cl_tr = cl_tmp[mask_tr]\n",
    "        cl_cal = cl_tmp[mask_cal]\n",
    "        \n",
    "        idx_cal_cp, idx_cal_gate, y_cal_cp, y_cal_gate, cl_cal_cp, cl_cal_gate = train_test_split(\n",
    "            idx_cal, y_cal, cl_cal,\n",
    "            test_size=0.5,\n",
    "            stratify=y_cal,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        y_cal_cp   = np.array(y_cal_cp)\n",
    "        y_cal_gate = np.array(y_cal_gate)\n",
    "        y_test     = np.array(y_test)\n",
    "        # mask des “réels” pour toutes les évaluations\n",
    "        mask_real = DF.loc[idx_test, \"source\"] == \"real\"\n",
    "\n",
    "        # 2) build features\n",
    "        X_tr        = build_X_(DF.loc[idx_tr],       curri_cols, static_cols, n)\n",
    "        X_cal_cp    = build_X_(DF.loc[idx_cal_cp],   curri_cols, static_cols, n)\n",
    "        X_cal_gate  = build_X_(DF.loc[idx_cal_gate], curri_cols, static_cols, n)\n",
    "        X_test      = build_X_(DF.loc[idx_test],     curri_cols, static_cols, n)\n",
    "        # 3) train base clf + calibrate for MCP\n",
    "        clf = clone(base_clf)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        calib = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\") \\\n",
    "                    .fit(X_cal_cp, y_cal_cp)\n",
    "\n",
    "        base_mapie = MapieClassifier(estimator=calib, method=\"lac\", cv=\"prefit\")\n",
    "        mond_mapie = MondrianCP(mapie_estimator=base_mapie) \\\n",
    "                        .fit(X_cal_cp, y_cal_cp, partition=cl_cal_cp)\n",
    "\n",
    "        # ---- MCP on TEST ----\n",
    "        _, yps_van_test = mond_mapie.predict(X_test, alpha=ALPHA, partition=cl_test)\n",
    "        pset_van_test = yps_van_test[:, :, 0]\n",
    "        cov_van = classification_coverage_score(y_test[mask_real], pset_van_test[mask_real])\n",
    "        wid_van = classification_mean_width_score(pset_van_test[mask_real])\n",
    "        covs_MCP.append(cov_van)\n",
    "        width_MCP.append(wid_van)\n",
    "        print(\"MCP\", cov_van, wid_van)\n",
    "        # ---- SPCI on TEST ----\n",
    "        model_spci = loaded_models[n - W]\n",
    "        pos_test   = DF.index.get_indexer(idx_test)\n",
    "        X_spci_test = X_array_hori[n - W + 1][pos_test]\n",
    "        intervals = [model_spci.predict_interval(x.reshape(1, -1))\n",
    "                     for x in X_spci_test]\n",
    "        L_preds, U_preds = zip(*intervals)\n",
    "\n",
    "        y_pred_bool_SPCI = np.zeros((len(intervals), 2), dtype=bool)\n",
    "        for i, (L, U) in enumerate(zip(L_preds, U_preds)):\n",
    "            if threshold > U:\n",
    "                y_pred_bool_SPCI[i, 1] = True\n",
    "            elif threshold < L:\n",
    "                y_pred_bool_SPCI[i, 0] = True\n",
    "            else:\n",
    "                y_pred_bool_SPCI[i, :] = True\n",
    "\n",
    "        cov_spci = classification_coverage_score(y_test[mask_real],\n",
    "                                                y_pred_bool_SPCI[mask_real])\n",
    "        wid_spci = classification_mean_width_score(y_pred_bool_SPCI[mask_real])\n",
    "        covs_SPCI.append(cov_spci)\n",
    "        width_SPCI.append(wid_spci)\n",
    "        print(\"SPCI\", cov_spci, wid_spci)\n",
    "        ##UNION \n",
    "        y_pred_bool_MCP = pset_van_test.astype(bool)\n",
    "        y_bool_union = y_pred_bool_MCP | y_pred_bool_SPCI\n",
    "        cov_union = classification_coverage_score(\n",
    "            y_test[mask_real],\n",
    "            y_bool_union[mask_real]\n",
    "        )\n",
    "        wid_union = classification_mean_width_score(\n",
    "            y_bool_union[mask_real]\n",
    "        )\n",
    "        covs_union.append(cov_union)\n",
    "        width_union.append(wid_union)\n",
    "        print(\"UNION :\", cov_union, wid_union)\n",
    "\n",
    "        # ---- construire la gate sur CAL_GATE ----\n",
    "        #  a) MCP predictions sur X_cal_gate\n",
    "        _, yps_van_gate = mond_mapie.predict(\n",
    "            X_cal_gate, alpha=ALPHA, partition=cl_cal_gate\n",
    "        )\n",
    "        pset_cal_cls = yps_van_gate[:, :, 0]\n",
    "\n",
    "        #  b) SPCI predictions sur X_cal_gate\n",
    "        pos_cal_gate  = DF.index.get_indexer(idx_cal_gate)\n",
    "        X_spci_cal    = X_array_hori[n - W + 1][pos_cal_gate]\n",
    "        intervals_cal = [model_spci.predict_interval(x.reshape(1, -1))\n",
    "                         for x in X_spci_cal]\n",
    "        L_cal, U_cal  = zip(*intervals_cal)\n",
    "\n",
    "        pset_cal_spc = np.zeros_like(pset_cal_cls, dtype=bool)\n",
    "        for i, (L, U) in enumerate(zip(L_cal, U_cal)):\n",
    "            if threshold > U:\n",
    "                pset_cal_spc[i, 1] = True\n",
    "            elif threshold < L:\n",
    "                pset_cal_spc[i, 0] = True\n",
    "            else:\n",
    "                pset_cal_spc[i, :] = True\n",
    "\n",
    "        #  c) préparer méta-features & labels pour la gate\n",
    "        df_sel_arr = []\n",
    "        labels_g   = []                      # ← on initialise labels_g\n",
    "\n",
    "        for i in range(len(idx_cal_gate)):\n",
    "            feat_vec = X_cal_gate[i]\n",
    "            w_cls    = pset_cal_cls[i].sum()\n",
    "            w_spc    = pset_cal_spc[i].sum()\n",
    "            diff     = w_cls - w_spc\n",
    "            err_cls  = int(y_cal_gate[i] not in np.where(pset_cal_cls[i])[0])\n",
    "            err_spc  = int(y_cal_gate[i] not in np.where(pset_cal_spc[i])[0])\n",
    "            if   err_cls == 0 and err_spc == 1:\n",
    "                gate_y = 0\n",
    "            elif err_spc == 0 and err_cls == 1:\n",
    "                gate_y = 1\n",
    "            elif err_cls == 0 and err_spc == 0:\n",
    "                gate_y = 0 if w_cls < w_spc else 1\n",
    "            else:\n",
    "                gate_y = 2\n",
    "            labels_g.append(gate_y)           # ← on stocke le label\n",
    "\n",
    "            meta_vec = np.concatenate([\n",
    "                 feat_vec,\n",
    "                 [w_cls, w_spc, diff, err_cls, err_spc]\n",
    "            ])\n",
    "            df_sel_arr.append(meta_vec)\n",
    "\n",
    "        X_gate_train = np.vstack(df_sel_arr)\n",
    "        gate_clf.fit(X_gate_train, np.array(labels_g))\n",
    "        # ---- appliquer la gate sur TEST ----\n",
    "        meta_test_arr = []\n",
    "        for i in range(len(idx_test)):\n",
    "            # feat_vec est un array 1D de taille n_features\n",
    "            feat_vec = X_test[i]\n",
    "            w_cls = pset_van_test[i].sum()\n",
    "            w_spc = y_pred_bool_SPCI[i].sum()\n",
    "            diff = w_cls - w_spc\n",
    "            # on concatène feat_vec et les 5 features méta\n",
    "            meta_vec = np.concatenate([\n",
    "                feat_vec,\n",
    "                [w_cls, w_spc, diff, 0, 0]    # err_cls=0, err_spc=0\n",
    "            ])\n",
    "            meta_test_arr.append(meta_vec)\n",
    "\n",
    "        # on empile en matrice (n_test × n_features_meta)\n",
    "        X_gate_test = np.vstack(meta_test_arr)\n",
    "\n",
    "        # on prédit le choix de la gate\n",
    "        choices = gate_clf.predict(X_gate_test)\n",
    "        pset_final = np.zeros_like(pset_van_test, dtype=bool)\n",
    "        for i, choice in enumerate(choices):\n",
    "            if choice == 0:\n",
    "                pset_final[i] = y_pred_bool_MCP[i]\n",
    "            elif choice == 1:\n",
    "                pset_final[i] = y_pred_bool_SPCI[i]\n",
    "            else:\n",
    "                pset_final[i] = y_pred_bool_MCP[i] | y_pred_bool_SPCI[i]\n",
    "\n",
    "        cov_c = classification_coverage_score(y_test[mask_real],\n",
    "                                             pset_final[mask_real])\n",
    "        wid_c = classification_mean_width_score(pset_final[mask_real])\n",
    "        covs_comb.append(cov_c)\n",
    "        width_comb.append(wid_c)\n",
    "        print(\"COMBINED\", cov_c, wid_c)\n",
    "    # on agrège les métriques\n",
    "    n_vals = list(range(W, W + len(covs_MCP)))\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"model\":             [name] * len(n_vals),\n",
    "        \"n\":                 n_vals,\n",
    "        \"coverage_MCP\":      covs_MCP,\n",
    "        \"width_MCP\":         width_MCP,\n",
    "        \"coverage_SPCI\":     covs_SPCI,\n",
    "        \"width_SPCI\":        width_SPCI,\n",
    "        \"coverage_union\":    covs_union, \n",
    "        \"width_union\":       width_union,\n",
    "        \"coverage_combined\": covs_comb,\n",
    "        \"width_combined\":    width_comb,\n",
    "    })\n",
    "    res_fin.append(df_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
